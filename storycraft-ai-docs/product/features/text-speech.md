Yes, you **can absolutely use ElevenLabs' text-to-speech (TTS) features in a full-stack Next.js application**, whether you're streaming audio directly or generating and storing audio files. Here's how to **integrate both streaming and non-streaming TTS into your Next.js project** in a modular, maintainable way.

---

## ğŸ§© Integration Overview

Youâ€™ll typically implement this on the **backend (API routes)** of your Next.js app, since you need to **protect your ElevenLabs API key** and possibly interact with AWS S3.

Weâ€™ll break it down into these parts:

1. **Setup**
2. **Install dependencies**
3. **Create reusable TTS utility**
4. **Expose API routes (streaming & non-streaming)**
5. (Optional) Upload to **S3 + generate presigned URL**
6. **Frontend integration**

---

## 1. âœ… Project Setup (Next.js Full Stack)

Make sure your project is structured like this:

```
/my-nextjs-app
â”œâ”€â”€ /app
â”‚   â”œâ”€â”€ /api
â”‚   â”‚   â”œâ”€â”€ tts-file.ts
â”‚   â”‚   â””â”€â”€ tts-stream.ts
â”œâ”€â”€ /lib/actions
â”‚       â”œâ”€â”€ elevenlabs.ts
â”‚       â””â”€â”€ s3.ts (optional)
â”œâ”€â”€ .env.local
```

---

## 2. ğŸ“¦ Install Dependencies

Install required packages:

```bash
npm install @elevenlabs/elevenlabs-js dotenv uuid
npm install aws-sdk # if you plan to use S3
```

---

## 3. ğŸ”§ Create ElevenLabs Client (`lib/actions/elevenlabs.ts`)

```ts
// lib/actions/elevenlabs.ts
import { ElevenLabsClient } from "@elevenlabs/elevenlabs-js";
import { createWriteStream } from "fs";
import { v4 as uuid } from "uuid";
import path from "path";

const elevenlabs = new ElevenLabsClient({
  apiKey: process.env.ELEVENLABS_API_KEY!,
});

const VOICE_ID = "JBFqnCBsd6RMkjVDRZzb"; // Replace with your chosen voice ID

export async function createAudioFileFromText(text: string): Promise<string> {
  const audio = await elevenlabs.textToSpeech.convert(VOICE_ID, {
    modelId: "eleven_multilingual_v2",
    text,
    outputFormat: "mp3_44100_128",
    voiceSettings: {
      stability: 0,
      similarityBoost: 0,
      useSpeakerBoost: true,
      speed: 1.0,
    },
  });

  const fileName = `${uuid()}.mp3`;
  const filePath = path.resolve("./public/audio", fileName);
  const fileStream = createWriteStream(filePath);

  return new Promise((resolve, reject) => {
    audio.pipe(fileStream);
    fileStream.on("finish", () => resolve(`/audio/${fileName}`));
    fileStream.on("error", reject);
  });
}

export async function createAudioStreamFromText(text: string): Promise<Buffer> {
  const audioStream = await elevenlabs.textToSpeech.stream(VOICE_ID, {
    modelId: "eleven_multilingual_v2",
    text,
    outputFormat: "mp3_44100_128",
    voiceSettings: {
      stability: 0,
      similarityBoost: 1.0,
      useSpeakerBoost: true,
      speed: 1.0,
    },
  });

  const chunks: Buffer[] = [];
  for await (const chunk of audioStream) {
    chunks.push(chunk);
  }

  return Buffer.concat(chunks);
}
```

---

## 4. ğŸ§  API Routes

### **API Route 1 â€“ Generate and save file (`app/api/tts-file.ts`)**

```ts
import { NextApiRequest, NextApiResponse } from "next";
import { createAudioFileFromText } from "@/lib/actions/elevenlabs";

export default async function handler(
  req: NextApiRequest,
  res: NextApiResponse
) {
  const { text } = req.body;

  if (!text) {
    return res.status(400).json({ error: "Text is required" });
  }

  try {
    const fileUrl = await createAudioFileFromText(text);
    res.status(200).json({ url: fileUrl });
  } catch (error) {
    res.status(500).json({ error: "Failed to create audio file" });
  }
}
```

---

### **API Route 2 â€“ Stream audio directly (`app/api/tts-stream.ts`)**

```ts
import { NextApiRequest, NextApiResponse } from "next";
import { createAudioStreamFromText } from "@/lib/actions/elevenlabs";

export const config = {
  api: {
    responseLimit: false, // Allow streaming large audio
  },
};

export default async function handler(
  req: NextApiRequest,
  res: NextApiResponse
) {
  const { text } = req.query;

  if (!text || typeof text !== "string") {
    return res.status(400).json({ error: "Text is required" });
  }

  try {
    const audioBuffer = await createAudioStreamFromText(text);
    res.setHeader("Content-Type", "audio/mpeg");
    res.setHeader("Content-Disposition", 'inline; filename="output.mp3"');
    res.send(audioBuffer);
  } catch (error) {
    res.status(500).json({ error: "Failed to stream audio" });
  }
}
```

---

## 5. â˜ï¸ (Optional) AWS S3 Upload

If you want to **upload to S3 and generate signed URLs**, create a file `lib/actions/s3.ts` and I can walk you through that part too. Let me know.

---

## 6. ğŸ§‘â€ğŸ’» Frontend Example (Streaming)

```tsx
// app/index.tsx
import { useState } from "react";

export default function Home() {
  const [text, setText] = useState("");
  const [audioUrl, setAudioUrl] = useState("");

  const handleStream = async () => {
    const res = await fetch(`/api/tts-stream?text=${encodeURIComponent(text)}`);
    const blob = await res.blob();
    const url = URL.createObjectURL(blob);
    setAudioUrl(url);
  };

  return (
    <div>
      <h1>Text to Speech</h1>
      <textarea value={text} onChange={(e) => setText(e.target.value)} />
      <button onClick={handleStream}>Play Stream</button>
      {audioUrl && <audio controls src={audioUrl} autoPlay />}
    </div>
  );
}
```

---

## âœ… Summary

You now have:

- ğŸ“„ **Audio file generation API**
- ğŸ§ **Audio streaming API**
- ğŸ” API key secured in `.env.local`
- ğŸ§  Modular logic in `lib/actions/tts.ts`
- ğŸµ Simple frontend to test playback

---

## Want Bonus Features?

Let me know if you'd like to add:

- âœ… Uploading to S3
- ğŸ§ Voice picker from ElevenLabs API
- ğŸ“± PWA for offline TTS playback
- ğŸ“š Save transcripts/audio to database
